{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # üìä EDA Completa ‚Äì Base Tratada de Churn Banc√°rio\n",
    "\n",
    "\n",
    "\n",
    " Notebook-script para VSCode (Python + Jupyter) usando a base:\n",
    "\n",
    " `data/base_tratada.csv`\n",
    "\n",
    "\n",
    "\n",
    " Objetivos:\n",
    "\n",
    " - Carregar a base tratada\n",
    "\n",
    " - Verificar qualidade dos dados\n",
    "\n",
    " - Explorar distribui√ß√µes (num√©ricas e categ√≥ricas)\n",
    "\n",
    " - Analisar outliers\n",
    "\n",
    " - Ver correla√ß√µes com o target\n",
    "\n",
    " - Fazer PCA e clusteriza√ß√£o\n",
    "\n",
    " - Preparar base final de modelagem (`base_modelagem.csv`)\n",
    "\n",
    "\n",
    "\n",
    " Execute c√©lula a c√©lula no VSCode (Run Cell) para uma an√°lise did√°tica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 0. Imports e Configura√ß√µes Iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Tentativa de importar statsmodels (opcional)\n",
    "try:\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    from statsmodels.tools.tools import add_constant\n",
    "    HAS_STATSMODELS = True\n",
    "except ImportError:\n",
    "    HAS_STATSMODELS = False\n",
    "    variance_inflation_factor = None\n",
    "    add_constant = None\n",
    "\n",
    "# Tentativa de importar plotly (opcional)\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    HAS_PLOTLY = True\n",
    "except ImportError:\n",
    "    HAS_PLOTLY = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 1200)\n",
    "\n",
    "TARGET_COL = \"Attrition\"\n",
    "\n",
    "print(\"‚úÖ Imports conclu√≠dos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Fun√ß√µes Utilit√°rias de Data Quality e EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_quality_report(df, save_path=None):\n",
    "    \"\"\"\n",
    "    Verifica√ß√µes r√°pidas de qualidade de dados e, opcionalmente,\n",
    "    salva a base tratada em CSV.\n",
    "    \"\"\"\n",
    "    if df is None:\n",
    "        raise ValueError(\"DataFrame 'df' n√£o foi fornecido (None).\")\n",
    "\n",
    "    print(\"\\nüîç Verifica√ß√£o de Data Quality:\")\n",
    "    print(f\"- Registros: {len(df)}\")\n",
    "    print(f\"- Duplicados: {df.duplicated().sum()} registros\")\n",
    "\n",
    "    mem_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
    "    print(f\"- Mem√≥ria usada: {mem_mb:.2f} MB\")\n",
    "\n",
    "    # Colunas constantes\n",
    "    constant_cols = [col for col in df.columns if df[col].nunique(dropna=False) == 1]\n",
    "    if constant_cols:\n",
    "        print(f\"- Colunas constantes: {constant_cols}\")\n",
    "    else:\n",
    "        print(\"- Colunas constantes: nenhuma\")\n",
    "\n",
    "    # Missing values\n",
    "    total_missing = df.isnull().sum().sum()\n",
    "    print(f\"- Valores ausentes (total): {total_missing}\")\n",
    "    if total_missing:\n",
    "        missing_per_col = df.isnull().sum()\n",
    "        top_missing = (\n",
    "            missing_per_col[missing_per_col > 0]\n",
    "            .sort_values(ascending=False)\n",
    "            .head(10)\n",
    "        )\n",
    "        print(\"- Top colunas com NA:\")\n",
    "        for col, cnt in top_missing.items():\n",
    "            pct = cnt / len(df) * 100\n",
    "            print(f\"    - {col}: {cnt} ({pct:.2f}%)\")\n",
    "\n",
    "    # Zeros em colunas num√©ricas\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    zero_counts = {c: int((df[c] == 0).sum()) for c in num_cols if (df[c] == 0).any()}\n",
    "    if zero_counts:\n",
    "        print(\"- Contagens de zeros em colunas num√©ricas (at√© 10):\")\n",
    "        for i, (col, cnt) in enumerate(\n",
    "            sorted(zero_counts.items(), key=lambda x: -x[1])\n",
    "        ):\n",
    "            if i >= 10:\n",
    "                break\n",
    "            print(f\"    - {col}: {cnt}\")\n",
    "\n",
    "    # Salvar arquivo tratado opcionalmente\n",
    "    if save_path:\n",
    "        p = Path(save_path)\n",
    "        p.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(p, index=False)\n",
    "        print(f\"- Base salva em: {p.resolve()}\")\n",
    "\n",
    "\n",
    "def detect_outliers_iqr(df, column, threshold=1.5):\n",
    "    \"\"\"Detecta outliers usando o m√©todo IQR (quartis).\"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower = Q1 - threshold * IQR\n",
    "    upper = Q3 + threshold * IQR\n",
    "\n",
    "    outliers = df[(df[column] < lower) | (df[column] > upper)]\n",
    "    outlier_pct = len(outliers) / len(df) * 100\n",
    "\n",
    "    return {\n",
    "        \"outlier_count\": int(len(outliers)),\n",
    "        \"outlier_percentage\": float(outlier_pct),\n",
    "        \"lower_bound\": float(lower),\n",
    "        \"upper_bound\": float(upper),\n",
    "        \"Q1\": float(Q1),\n",
    "        \"Q3\": float(Q3),\n",
    "        \"IQR\": float(IQR),\n",
    "        \"min\": float(df[column].min()),\n",
    "        \"max\": float(df[column].max()),\n",
    "        \"mean\": float(df[column].mean()),\n",
    "        \"median\": float(df[column].median()),\n",
    "    }\n",
    "\n",
    "\n",
    "def detailed_statistical_comparison(df, numeric_cols, target_col=TARGET_COL):\n",
    "    \"\"\"Compara estat√≠sticas das vari√°veis num√©ricas entre as classes do target.\"\"\"\n",
    "    stats_comparison = pd.DataFrame()\n",
    "\n",
    "    for col in numeric_cols:\n",
    "        # Estat√≠sticas por grupo\n",
    "        group_stats = df.groupby(target_col)[col].agg(\n",
    "            [\n",
    "                \"mean\",\n",
    "                \"median\",\n",
    "                \"std\",\n",
    "                \"min\",\n",
    "                \"max\",\n",
    "                \"skew\",\n",
    "                lambda x: x.quantile(0.75) - x.quantile(0.25),\n",
    "            ]\n",
    "        ).rename(columns={\"<lambda_0>\": \"IQR\"})\n",
    "\n",
    "        group0 = df[df[target_col] == 0][col].dropna()\n",
    "        group1 = df[df[target_col] == 1][col].dropna()\n",
    "\n",
    "        if len(group0) > 1 and len(group1) > 1:\n",
    "            t_stat, p_value = stats.ttest_ind(group0, group1, equal_var=False)\n",
    "        else:\n",
    "            t_stat, p_value = np.nan, np.nan\n",
    "\n",
    "        cv0 = group0.std() / group0.mean() if group0.mean() != 0 else np.nan\n",
    "        cv1 = group1.std() / group1.mean() if group1.mean() != 0 else np.nan\n",
    "\n",
    "        stats_comparison[col] = {\n",
    "            \"t_statistic\": t_stat,\n",
    "            \"p_value\": p_value,\n",
    "            \"mean_diff\": group_stats.loc[1, \"mean\"] - group_stats.loc[0, \"mean\"],\n",
    "            \"cv_0\": cv0,\n",
    "            \"cv_1\": cv1,\n",
    "        }\n",
    "\n",
    "    return stats_comparison.T\n",
    "\n",
    "\n",
    "def significant_correlations(df, cols, target_col=TARGET_COL, alpha=0.05):\n",
    "    \"\"\"Calcula correla√ß√£o de Pearson + p-valor entre features num√©ricas e o target.\"\"\"\n",
    "    results = []\n",
    "    for col in cols:\n",
    "        if col == target_col:\n",
    "            continue\n",
    "\n",
    "        x = df[col].dropna()\n",
    "        y = df[target_col].loc[x.index].dropna()\n",
    "\n",
    "        if len(x) > 1 and len(y) > 1:\n",
    "            corr, p_value = stats.pearsonr(x, y)\n",
    "        else:\n",
    "            corr, p_value = np.nan, np.nan\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"feature\": col,\n",
    "                \"correlation\": corr,\n",
    "                \"p_value\": p_value,\n",
    "                \"significant\": bool(p_value < alpha)\n",
    "                if not np.isnan(p_value)\n",
    "                else False,\n",
    "                \"abs_correlation\": abs(corr) if not np.isnan(corr) else np.nan,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return pd.DataFrame(results).sort_values(\"abs_correlation\", ascending=False)\n",
    "\n",
    "\n",
    "def calculate_vif(df, numeric_cols):\n",
    "    \"\"\"Calcula VIF (Variance Inflation Factor) para avaliar multicolinearidade.\"\"\"\n",
    "    if not HAS_STATSMODELS:\n",
    "        raise ImportError(\n",
    "            \"statsmodels n√£o est√° instalado. Instale com: pip install statsmodels\"\n",
    "        )\n",
    "\n",
    "    X = add_constant(df[numeric_cols].dropna())\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = X.columns\n",
    "    vif_data[\"VIF\"] = [\n",
    "        variance_inflation_factor(X.values, i) for i in range(X.shape[1])\n",
    "    ]\n",
    "    return vif_data.sort_values(\"VIF\", ascending=False)\n",
    "\n",
    "\n",
    "def pca_3d_visualization(X_scaled, target, n_components=3):\n",
    "    \"\"\"PCA em 3D para visualiza√ß√£o dos clientes coloridos por churn.\"\"\"\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "    from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    scatter = ax.scatter(\n",
    "        X_pca[:, 0],\n",
    "        X_pca[:, 1],\n",
    "        X_pca[:, 2],\n",
    "        c=target,\n",
    "        cmap=\"viridis\",\n",
    "        alpha=0.6,\n",
    "        s=15,\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]:.1%})\")\n",
    "    ax.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]:.1%})\")\n",
    "    ax.set_zlabel(f\"PC3 ({pca.explained_variance_ratio_[2]:.1%})\")\n",
    "    plt.title(\"PCA 3D - Clientes por Status de Churn\")\n",
    "    plt.legend(*scatter.legend_elements(), title=\"Churn\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return X_pca, pca\n",
    "\n",
    "\n",
    "def elbow_method(X_scaled, max_clusters=10):\n",
    "    \"\"\"M√©todo do cotovelo para escolher k no KMeans.\"\"\"\n",
    "    inertias = []\n",
    "    for k in range(2, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(X_scaled)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(2, max_clusters + 1), inertias, marker=\"o\")\n",
    "    plt.xlabel(\"N√∫mero de Clusters (k)\")\n",
    "    plt.ylabel(\"In√©rcia (Within-Cluster SSE)\")\n",
    "    plt.title(\"M√©todo do Cotovelo para KMeans\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def silhouette_analysis(X_scaled, max_clusters=10):\n",
    "    \"\"\"Silhouette score por n√∫mero de clusters.\"\"\"\n",
    "    scores = []\n",
    "    for k in range(2, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(X_scaled)\n",
    "        score = silhouette_score(X_scaled, labels)\n",
    "        scores.append(score)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(range(2, max_clusters + 1), scores, marker=\"o\", color=\"tab:red\")\n",
    "    plt.xlabel(\"N√∫mero de Clusters (k)\")\n",
    "    plt.ylabel(\"Silhouette Score\")\n",
    "    plt.title(\"An√°lise de Silhouette para KMeans\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def generate_eda_report(df, target_col=TARGET_COL):\n",
    "    \"\"\"Gera um dicion√°rio com resumo de EDA para exportar em JSON.\"\"\"\n",
    "    report = {\n",
    "        \"dataset_shape\": df.shape,\n",
    "        \"target_distribution\": df[target_col].value_counts().to_dict()\n",
    "        if target_col in df.columns\n",
    "        else None,\n",
    "        \"missing_values_total\": int(df.isnull().sum().sum()),\n",
    "        \"duplicates\": int(df.duplicated().sum()),\n",
    "        \"numeric_features\": df.select_dtypes(include=[np.number]).columns.tolist(),\n",
    "        \"categorical_features\": df.select_dtypes(include=[\"object\"]).columns.tolist(),\n",
    "    }\n",
    "\n",
    "    report[\"numeric_stats\"] = df.describe().T.to_dict()\n",
    "\n",
    "    if target_col in df.columns:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        correlations = (\n",
    "            df[numeric_cols].corr()[target_col].sort_values(ascending=False)\n",
    "        )\n",
    "        report[\"top_correlations\"] = correlations.head(10).to_dict()\n",
    "\n",
    "    return report\n",
    "\n",
    "\n",
    "def export_analysis_results(df, cluster_labels, pca_result, output_dir=\"eda_results\"):\n",
    "    \"\"\"Exporta resultados principais de EDA para CSVs em uma pasta.\"\"\"\n",
    "    import os\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    df_export = df.copy()\n",
    "    if cluster_labels is not None:\n",
    "        df_export[\"Cluster\"] = cluster_labels\n",
    "    if pca_result is not None and pca_result.shape[1] >= 2:\n",
    "        df_export[\"PC1\"] = pca_result[:, 0]\n",
    "        df_export[\"PC2\"] = pca_result[:, 1]\n",
    "\n",
    "    df_export.to_csv(f\"{output_dir}/dataset_with_clusters.csv\", index=False)\n",
    "\n",
    "    df.describe().to_csv(f\"{output_dir}/statistical_summary.csv\")\n",
    "\n",
    "    print(f\"‚úÖ Resultados exportados para '{output_dir}/'\")\n",
    "\n",
    "\n",
    "def interactive_distribution(df, col, target_col=TARGET_COL):\n",
    "    \"\"\"Histograma interativo por classe (se Plotly estiver instalado).\"\"\"\n",
    "    if not HAS_PLOTLY:\n",
    "        print(\"‚ö†Ô∏è Plotly n√£o est√° instalado. Use: pip install plotly\")\n",
    "        return\n",
    "\n",
    "    fig = px.histogram(\n",
    "        df,\n",
    "        x=col,\n",
    "        color=target_col,\n",
    "        marginal=\"box\",\n",
    "        nbins=50,\n",
    "        barmode=\"overlay\",\n",
    "        opacity=0.7,\n",
    "        title=f\"Distribui√ß√£o de {col} por Churn\",\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def visualize_outliers(df, column, target_col=TARGET_COL, outlier_stats=None):\n",
    "    \"\"\"Boxplots geral e por classe, com impress√£o das estat√≠sticas de outliers.\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    sns.boxplot(data=df, y=column, ax=axes[0])\n",
    "    axes[0].set_title(f\"Boxplot de {column}\")\n",
    "\n",
    "    if target_col in df.columns:\n",
    "        sns.boxplot(data=df, x=target_col, y=column, ax=axes[1])\n",
    "        axes[1].set_title(f\"{column} por Status (Attrition)\")\n",
    "    else:\n",
    "        axes[1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if outlier_stats is not None and column in outlier_stats:\n",
    "        stats_col = outlier_stats[column]\n",
    "        print(f\"\\nüìà Estat√≠sticas de {column}:\")\n",
    "        for k, v in stats_col.items():\n",
    "            print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Carregamento da Base `data/base_tratada.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path.cwd()\n",
    "if PROJECT_ROOT.name.lower() in {\"notebooks\", \"nb\"}:\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "DATA_PATH = PROJECT_ROOT / \"data\" / \"base_tratada.csv\"\n",
    "\n",
    "print(\"üìÅ Projeto em:\", PROJECT_ROOT)\n",
    "print(\"üìÑ Procurando base em:\", DATA_PATH)\n",
    "\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Arquivo base_tratada.csv n√£o encontrado em {DATA_PATH}.\\n\"\n",
    "        \"Verifique se est√° rodando o notebook a partir da pasta raiz do projeto \"\n",
    "        \"(Bank-Churn-Prediction-montes_claros) ou ajuste o caminho manualmente.\"\n",
    "    )\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"‚úÖ Base carregada! Formato:\", df.shape)\n",
    "display(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Data Quality R√°pido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_quality_report(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Identifica√ß√£o de Vari√°veis Num√©ricas e Categ√≥ricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "if TARGET_COL in numeric_cols:\n",
    "    numeric_cols.remove(TARGET_COL)\n",
    "\n",
    "cat_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "\n",
    "print(\"üî¢ Vari√°veis num√©ricas:\", numeric_cols)\n",
    "print(\"üî† Vari√°veis categ√≥ricas:\", cat_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Distribui√ß√£o da Vari√°vel Target (Churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TARGET_COL in df.columns:\n",
    "    churn_counts = df[TARGET_COL].value_counts().sort_index()\n",
    "    churn_percent = churn_counts / churn_counts.sum() * 100\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "    sns.barplot(x=churn_counts.index, y=churn_counts.values, ax=ax[0])\n",
    "    ax[0].set_title(\"Distribui√ß√£o Absoluta de Classes\")\n",
    "    ax[0].set_xlabel(\"Attrition (0 = ativo, 1 = churn)\")\n",
    "    ax[0].set_ylabel(\"Quantidade\")\n",
    "\n",
    "    sns.barplot(x=churn_percent.index, y=churn_percent.values, ax=ax[1])\n",
    "    ax[1].set_title(\"Distribui√ß√£o Percentual de Classes\")\n",
    "    ax[1].set_xlabel(\"Attrition\")\n",
    "    ax[1].set_ylabel(\"% de clientes\")\n",
    "\n",
    "    for i, p in enumerate(ax[1].patches):\n",
    "        ax[1].annotate(\n",
    "            f\"{churn_percent.values[i]:.1f}%\",\n",
    "            (p.get_x() + p.get_width() / 2, p.get_height()),\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nüìå Taxa de churn aproximada:\",\n",
    "          f\"{churn_percent.loc[1]:.2f}%\" if 1 in churn_percent.index else churn_percent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6. An√°lise de Outliers (IQR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_summary = {}\n",
    "\n",
    "for col in numeric_cols:\n",
    "    outlier_summary[col] = detect_outliers_iqr(df, col)\n",
    "\n",
    "outlier_df = pd.DataFrame(outlier_summary).T\n",
    "outlier_df = outlier_df.sort_values(\"outlier_percentage\", ascending=False)\n",
    "\n",
    "print(\"Resumo de outliers (top 10):\")\n",
    "display(outlier_df.head(10))\n",
    "\n",
    "significant_outliers = outlier_df[outlier_df[\"outlier_percentage\"] > 1]\n",
    "print(\"\\nVari√°veis com mais de 1% de outliers:\", len(significant_outliers))\n",
    "display(significant_outliers.head(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Visualiza√ß√£o dos principais outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in significant_outliers.head(3).index:\n",
    "    visualize_outliers(df, col, target_col=TARGET_COL, outlier_stats=outlier_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Outliers por classe (churn vs n√£o churn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TARGET_COL in df.columns:\n",
    "    outliers_by_class = {}\n",
    "\n",
    "    for col in significant_outliers.head(5).index:\n",
    "        churn_0 = df[df[TARGET_COL] == 0][col]\n",
    "        churn_1 = df[df[TARGET_COL] == 1][col]\n",
    "\n",
    "        stats_0 = detect_outliers_iqr(pd.DataFrame({col: churn_0}), col)\n",
    "        stats_1 = detect_outliers_iqr(pd.DataFrame({col: churn_1}), col)\n",
    "\n",
    "        outliers_by_class[col] = {\n",
    "            \"classe_0_pct\": stats_0[\"outlier_percentage\"],\n",
    "            \"classe_1_pct\": stats_1[\"outlier_percentage\"],\n",
    "            \"diferen√ßa\": stats_1[\"outlier_percentage\"] - stats_0[\"outlier_percentage\"],\n",
    "        }\n",
    "\n",
    "    outliers_class_df = pd.DataFrame(outliers_by_class).T.sort_values(\n",
    "        \"diferen√ßa\", ascending=False\n",
    "    )\n",
    "\n",
    "    print(\"Diferen√ßa na porcentagem de outliers entre classes:\")\n",
    "    display(outliers_class_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 7. Correla√ß√µes com o Target e VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_results = significant_correlations(df, numeric_cols, target_col=TARGET_COL)\n",
    "print(\"Top correla√ß√µes (em valor absoluto) com Attrition:\")\n",
    "display(corr_results.head(15))\n",
    "\n",
    "if HAS_STATSMODELS:\n",
    "    try:\n",
    "        vif_df = calculate_vif(df, numeric_cols)\n",
    "        print(\"\\nVIF (multicolinearidade) ‚Äì top 15:\")\n",
    "        display(vif_df.head(15))\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Erro ao calcular VIF:\", e)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è statsmodels n√£o est√° instalado. Pulei o c√°lculo de VIF.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 8. PCA 2D e 3D + Clusteriza√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalonar apenas vari√°veis num√©ricas\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "# PCA 2D\n",
    "pca_2d = PCA(n_components=2, random_state=42)\n",
    "X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "df_pca = pd.DataFrame(X_pca_2d, columns=[\"PC1\", \"PC2\"])\n",
    "if TARGET_COL in df.columns:\n",
    "    df_pca[\"Attrition\"] = df[TARGET_COL]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.scatterplot(\n",
    "    data=df_pca.sample(n=min(3000, len(df_pca)), random_state=42),\n",
    "    x=\"PC1\",\n",
    "    y=\"PC2\",\n",
    "    hue=\"Attrition\" if \"Attrition\" in df_pca.columns else None,\n",
    "    alpha=0.6,\n",
    ")\n",
    "plt.title(\"PCA 2D ‚Äì Clientes por Status de Churn\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# PCA 3D\n",
    "if len(numeric_cols) >= 3:\n",
    "    X_pca_3d, pca_model_3d = pca_3d_visualization(\n",
    "        X_scaled,\n",
    "        df[TARGET_COL] if TARGET_COL in df.columns else pd.Series([0] * len(df)),\n",
    "    )\n",
    "else:\n",
    "    X_pca_3d, pca_model_3d = None, None\n",
    "    print(\"PCA 3D n√£o executado: menos de 3 vari√°veis num√©ricas.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Escolha de k com cotovelo e silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbow_method(X_scaled, max_clusters=8)\n",
    "silhouette_analysis(X_scaled, max_clusters=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Clusteriza√ß√£o com KMeans (k = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "df_clusters = df.copy()\n",
    "df_clusters[\"Cluster\"] = cluster_labels\n",
    "\n",
    "cols_for_profile = numeric_cols.copy()\n",
    "if TARGET_COL in df.columns:\n",
    "    cols_for_profile.append(TARGET_COL)\n",
    "\n",
    "cluster_profile = (\n",
    "    df_clusters.groupby(\"Cluster\")[cols_for_profile]\n",
    "    .mean()\n",
    "    .round(3)\n",
    ")\n",
    "\n",
    "print(\"Perfil m√©dio dos clusters (vari√°veis num√©ricas + churn):\")\n",
    "display(cluster_profile)\n",
    "\n",
    "if TARGET_COL in df.columns:\n",
    "    cluster_churn = (\n",
    "        df_clusters.groupby(\"Cluster\")[TARGET_COL]\n",
    "        .agg([\"mean\", \"count\"])\n",
    "        .rename(columns={\"mean\": \"taxa_churn\", \"count\": \"qtd_clientes\"})\n",
    "        .round(3)\n",
    "    )\n",
    "    print(\"\\nTaxa de churn por cluster:\")\n",
    "    display(cluster_churn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 9. Exportar Relat√≥rios de EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_report = generate_eda_report(df, target_col=TARGET_COL)\n",
    "with open(PROJECT_ROOT / \"eda_report.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(eda_report, f, indent=4, ensure_ascii=False)\n",
    "print(\"üìÅ Arquivo 'eda_report.json' salvo.\")\n",
    "\n",
    "export_analysis_results(\n",
    "    df,\n",
    "    cluster_labels=cluster_labels,\n",
    "    pca_result=X_pca_2d,\n",
    "    output_dir=PROJECT_ROOT / \"eda_results\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ EDA conclu√≠da.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 10. Preparar Base Final de Modelagem\n",
    "\n",
    "\n",
    "\n",
    " Aqui removemos vari√°veis altamente multicolineares e adicionamos\n",
    "\n",
    " o r√≥tulo de Cluster como feature categ√≥rica. Salvamos:\n",
    "\n",
    "\n",
    "\n",
    " - `data/base_modelagem.csv`\n",
    "\n",
    " - `data/features_modelagem.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colunas multicolineares para remover (redundantes entre si)\n",
    "cols_to_drop = [\n",
    "    \"Credit_Limit\",\n",
    "    \"Total_Revolving_Bal\",\n",
    "    \"Avg_Open_To_Buy\",\n",
    "    \"Rotativo_Ratio\",\n",
    "    \"Disponibilidade_Relativa\",\n",
    "]\n",
    "\n",
    "df_model = df_clusters.drop(columns=[c for c in cols_to_drop if c in df_clusters.columns])\n",
    "\n",
    "# Garante tipo inteiro da vari√°vel alvo\n",
    "if TARGET_COL in df_model.columns:\n",
    "    df_model[TARGET_COL] = df_model[TARGET_COL].astype(int)\n",
    "\n",
    "# Defini√ß√£o das features num√©ricas e categ√≥ricas finais\n",
    "NUM_FEATURES_MODEL = [\n",
    "    \"Customer_Age\",\n",
    "    \"Dependent_count\",\n",
    "    \"Months_on_book\",\n",
    "    \"Total_Relationship_Count\",\n",
    "    \"Months_Inactive_12_mon\",\n",
    "    \"Contacts_Count_12_mon\",\n",
    "    \"Total_Amt_Chng_Q4_Q1\",\n",
    "    \"Total_Trans_Amt\",\n",
    "    \"Total_Trans_Ct\",\n",
    "    \"Total_Ct_Chng_Q4_Q1\",\n",
    "    \"Avg_Utilization_Ratio\",\n",
    "    \"Ticket_Medio\",\n",
    "    \"Transacoes_por_Mes\",\n",
    "    \"Gasto_Medio_Mensal\",\n",
    "]\n",
    "\n",
    "CAT_FEATURES_MODEL = [\n",
    "    \"Gender\",\n",
    "    \"Education_Level\",\n",
    "    \"Marital_Status\",\n",
    "    \"Income_Category\",\n",
    "    \"Card_Category\",\n",
    "    \"Faixa_Idade\",\n",
    "    \"Renda_Class\",\n",
    "    \"Cluster\",\n",
    "]\n",
    "\n",
    "# Salvar base de modelagem\n",
    "OUTPUT_MODEL_PATH = PROJECT_ROOT / \"data\" / \"base_modelagem.csv\"\n",
    "df_model.to_csv(OUTPUT_MODEL_PATH, index=False, encoding=\"utf-8\")\n",
    "print(f\"‚úÖ Base de modelagem salva em: {OUTPUT_MODEL_PATH}\")\n",
    "\n",
    "# Salvar features em JSON\n",
    "features_info = {\n",
    "    \"numeric_features\": NUM_FEATURES_MODEL,\n",
    "    \"categorical_features\": CAT_FEATURES_MODEL,\n",
    "    \"target\": TARGET_COL,\n",
    "}\n",
    "\n",
    "FEATURES_JSON_PATH = PROJECT_ROOT / \"data\" / \"features_modelagem.json\"\n",
    "with open(FEATURES_JSON_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(features_info, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"üìÅ Arquivo 'features_modelagem.json' salvo em: {FEATURES_JSON_PATH}\")\n",
    "\n",
    "print(\"\\n‚úÖ Pipeline de EDA + prepara√ß√£o de base de modelagem finalizado.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
