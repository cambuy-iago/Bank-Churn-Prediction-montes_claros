{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ebfd548",
   "metadata": {},
   "source": [
    "\n",
    "# üìò Feature_Importance_SHAP.ipynb\n",
    "# Etapa 1: Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd83fcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import sys\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adf3643",
   "metadata": {},
   "source": [
    "# Importa configura√ß√µes do projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3453810",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from src.config import BASE_DIR, DATA_PATH, FIGURES_PATH, MODELS_PATH\n",
    "    print(f\"‚úÖ Configura√ß√µes importadas do config.py\")\n",
    "    print(f\"üìÅ Base do projeto: {BASE_DIR}\")\n",
    "except (ModuleNotFoundError, ImportError) as e:\n",
    "    print(f\"‚ö†Ô∏è  Erro ao importar config.py: {e}\")\n",
    "    # Fallback: define caminhos manualmente\n",
    "    BASE_DIR = Path.cwd()\n",
    "    if BASE_DIR.name == 'notebooks':\n",
    "        BASE_DIR = BASE_DIR.parent\n",
    "    DATA_PATH = BASE_DIR / \"data\" / \"BankChurners.csv\"\n",
    "    FIGURES_PATH = BASE_DIR / \"reports\" / \"figures\"\n",
    "    MODELS_PATH = BASE_DIR / \"models\"\n",
    "    FIGURES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"üìÅ BASE_DIR definido como: {BASE_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bdc2c5",
   "metadata": {},
   "source": [
    "# Etapa 2: Fun√ß√£o de feature engineering (inline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601f2a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Etapa 2: Fun√ß√£o de feature engineering (inline)\n",
    "def criar_variaveis_derivadas(df):\n",
    "    \"\"\"\n",
    "    Cria vari√°veis derivadas para melhorar o poder preditivo do modelo\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # LTV Proxy (Lifetime Value aproximado)\n",
    "    df['LTV_Proxy'] = df['Customer_Age'] * df['Total_Trans_Amt']\n",
    "    \n",
    "    # Raz√£o de transa√ß√µes por relacionamento\n",
    "    df['Trans_Per_Month'] = df['Total_Trans_Ct'] / df['Months_on_book']\n",
    "    \n",
    "    # Ticket m√©dio\n",
    "    df['Avg_Transaction_Value'] = df['Total_Trans_Amt'] / (df['Total_Trans_Ct'] + 1)\n",
    "    \n",
    "    # Utiliza√ß√£o ajustada por limite\n",
    "    df['Utilization_Efficiency'] = df['Avg_Utilization_Ratio'] * df['Credit_Limit']\n",
    "    \n",
    "    # Idade do cart√£o vs idade do cliente\n",
    "    df['Card_Age_Ratio'] = df['Months_on_book'] / (df['Customer_Age'] + 1)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95ace77",
   "metadata": {},
   "source": [
    "# Etapa 3: Leitura e prepara√ß√£o dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fbf6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PREPARA√á√ÉO DOS DADOS\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c011fd",
   "metadata": {},
   "source": [
    "# Define caminhos usando as vari√°veis do config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f4940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tratada_path = BASE_DIR / \"data\" / \"base_tratada.csv\"\n",
    "bank_churners_path = DATA_PATH  # J√° definido no config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19863638",
   "metadata": {},
   "source": [
    "# Verifica qual arquivo usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aa8d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_raw = False\n",
    "if base_tratada_path.exists():\n",
    "    file_path = base_tratada_path\n",
    "    print(f\"‚úÖ Usando base tratada: {file_path.name}\")\n",
    "elif bank_churners_path.exists():\n",
    "    file_path = bank_churners_path\n",
    "    use_raw = True\n",
    "    print(f\"‚úÖ Usando base raw: {file_path.name}\")\n",
    "else:\n",
    "    # Debug: mostra o que existe no diret√≥rio data/\n",
    "    data_dir = BASE_DIR / \"data\"\n",
    "    if data_dir.exists():\n",
    "        print(f\"\\nüìÇ Conte√∫do de {data_dir}:\")\n",
    "        for item in data_dir.iterdir():\n",
    "            print(f\"   - {item.name}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Diret√≥rio n√£o encontrado: {data_dir}\")\n",
    "    \n",
    "    raise FileNotFoundError(\n",
    "        f\"\\n‚ùå Nenhum arquivo de dados encontrado!\\n\"\n",
    "        f\"   Procurado em:\\n\"\n",
    "        f\"   - {base_tratada_path}\\n\"\n",
    "        f\"   - {bank_churners_path}\\n\"\n",
    "        f\"\\nüí° Certifique-se de que um desses arquivos existe no diret√≥rio data/\"\n",
    "    )\n",
    "\n",
    "print(f\"üìÇ Lendo arquivo: {file_path.resolve()}\")\n",
    "df = pd.read_csv(file_path)\n",
    "print(f\"‚úÖ Arquivo carregado! Shape inicial: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aaa6cd",
   "metadata": {},
   "source": [
    "# Se for o arquivo raw, aplica feature engineering e prepara target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817084e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_raw:\n",
    "    print(\"\\nüîß Aplicando feature engineering...\")\n",
    "    df = criar_variaveis_derivadas(df)\n",
    "    \n",
    "    # Remove colunas desnecess√°rias (√∫ltimas 2 s√£o do Naive Bayes)\n",
    "    df = df.iloc[:, :-2]\n",
    "    \n",
    "    # Cria vari√°vel target\n",
    "    if 'Attrition_Flag' in df.columns:\n",
    "        df['Attrition'] = (df['Attrition_Flag'] == 'Attrited Customer').astype(int)\n",
    "        df = df.drop('Attrition_Flag', axis=1)\n",
    "    \n",
    "    print(f\"‚úÖ Feature engineering conclu√≠do! Nova shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42563a10",
   "metadata": {},
   "source": [
    "# Etapa 4: Sele√ß√£o de vari√°veis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e4a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PREPARA√á√ÉO DO MODELO\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dc1e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'Customer_Age', 'Dependent_count', 'Credit_Limit', \n",
    "    'Total_Trans_Amt', 'Total_Trans_Ct',\n",
    "    'Avg_Utilization_Ratio', 'Total_Ct_Chng_Q4_Q1', 'Total_Amt_Chng_Q4_Q1',\n",
    "    'LTV_Proxy', 'Trans_Per_Month', 'Avg_Transaction_Value'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241bf385",
   "metadata": {},
   "source": [
    "# Filtra apenas features que existem no dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9776cbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_features = [f for f in features if f in df.columns]\n",
    "print(f\"üìä Features dispon√≠veis: {len(available_features)}/{len(features)}\")\n",
    "print(f\"   {', '.join(available_features)}\")\n",
    "\n",
    "X = df[available_features]\n",
    "y = df['Attrition']\n",
    "\n",
    "print(f\"\\nüìà Distribui√ß√£o do target:\")\n",
    "print(f\"   N√£o-Churn: {(y==0).sum()} ({(y==0).sum()/len(y)*100:.1f}%)\")\n",
    "print(f\"   Churn: {(y==1).sum()} ({(y==1).sum()/len(y)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eece47f0",
   "metadata": {},
   "source": [
    "# Split estratificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3026ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.3, random_state=42\n",
    ")\n",
    "print(f\"\\n‚úÖ Split realizado:\")\n",
    "print(f\"   Treino: {X_train.shape[0]} amostras\")\n",
    "print(f\"   Teste: {X_test.shape[0]} amostras\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bda06a",
   "metadata": {},
   "source": [
    "# Etapa 5: Treinamento do modelo XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ebee67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TREINAMENTO DO MODELO\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Configura√ß√£o expl√≠cita para compatibilidade com SHAP\n",
    "model = xgb.XGBClassifier(\n",
    "    use_label_encoder=False, \n",
    "    eval_metric=\"logloss\", \n",
    "    random_state=42,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    base_score=0.5  # Define explicitamente como float para compatibilidade com SHAP\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"‚úÖ Modelo XGBoost treinado com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168ec4db",
   "metadata": {},
   "source": [
    "# Avalia√ß√£o b√°sica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab8b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"\\nüìä Performance do modelo:\")\n",
    "print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "print(f\"   AUC-ROC: {auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee45c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import shap\n",
    "    print(\"‚úÖ Biblioteca SHAP carregada com sucesso!\")\n",
    "    \n",
    "    print(\"üîç Calculando valores SHAP...\")\n",
    "    \n",
    "    # Limpa dados de teste\n",
    "    X_test_clean = X_test.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "    \n",
    "    # Tenta usar TreeExplainer com tratamento de erro\n",
    "    try:\n",
    "        # Usa TreeExplainer espec√≠fico para XGBoost\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_test_clean)\n",
    "        \n",
    "        # Para classifica√ß√£o bin√°ria, pega apenas os valores da classe positiva\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values_display = shap_values[1]  # Classe 1 (Churn)\n",
    "        else:\n",
    "            shap_values_display = shap_values\n",
    "            \n",
    "    except (ValueError, AttributeError) as e:\n",
    "        print(f\"‚ö†Ô∏è  TreeExplainer falhou: {str(e)[:100]}\")\n",
    "        print(\"   Tentando m√©todo alternativo com predict...\")\n",
    "        \n",
    "        # M√©todo alternativo: usa modelo como fun√ß√£o de predi√ß√£o\n",
    "        explainer = shap.Explainer(model.predict_proba, X_train)\n",
    "        shap_values_obj = explainer(X_test_clean)\n",
    "        shap_values_display = shap_values_obj.values[:, 1]  # Classe positiva\n",
    "    \n",
    "    print(\"‚úÖ Valores SHAP calculados!\")\n",
    "    \n",
    "    # Usa FIGURES_PATH do config.py\n",
    "    print(f\"\\nüìä Salvando gr√°ficos em: {FIGURES_PATH}\")\n",
    "    \n",
    "    # Visualiza√ß√£o 1: Summary Plot\n",
    "    print(\"\\nüìä Gerando Summary Plot...\")\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    shap.summary_plot(shap_values_display, X_test_clean, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_PATH / \"shap_summary_plot.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"   ‚úÖ Salvo em: {FIGURES_PATH / 'shap_summary_plot.png'}\")\n",
    "    \n",
    "    # Visualiza√ß√£o 2: Bar Plot\n",
    "    print(\"\\nüìä Gerando Bar Plot...\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.summary_plot(shap_values_display, X_test_clean, plot_type=\"bar\", show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_PATH / \"shap_bar_plot.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(f\"   ‚úÖ Salvo em: {FIGURES_PATH / 'shap_bar_plot.png'}\")\n",
    "    \n",
    "    # Visualiza√ß√£o 3: Dependence Plot para feature mais importante\n",
    "    most_important_feature = X_test_clean.columns[\n",
    "        np.abs(shap_values_display).mean(0).argmax()\n",
    "    ]\n",
    "    print(f\"\\nüìä Gerando Dependence Plot para '{most_important_feature}'...\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.dependence_plot(\n",
    "        most_important_feature, \n",
    "        shap_values_display, \n",
    "        X_test_clean, \n",
    "        show=False\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        FIGURES_PATH / f\"shap_dependence_{most_important_feature}.png\", \n",
    "        dpi=300, \n",
    "        bbox_inches='tight'\n",
    "    )\n",
    "    plt.show()\n",
    "    print(f\"   ‚úÖ Salvo em: {FIGURES_PATH / f'shap_dependence_{most_important_feature}.png'}\")\n",
    "    \n",
    "    # Feature importance summary\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TOP 5 FEATURES MAIS IMPORTANTES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X_test_clean.columns,\n",
    "        'importance': np.abs(shap_values_display).mean(0)\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    for idx, row in feature_importance.head(5).iterrows():\n",
    "        print(f\"{row['feature']:30s} | {row['importance']:.4f}\")\n",
    "    \n",
    "except ModuleNotFoundError:\n",
    "    print(\"\\n‚ùå A biblioteca SHAP n√£o est√° instalada!\")\n",
    "    print(\"   Execute: pip install shap\")\n",
    "    print(\"\\nüí° Continuando com feature importance do XGBoost...\")\n",
    "    \n",
    "    # Feature importance alternativa usando XGBoost nativo\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': available_features,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['feature'], importance_df['importance'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Feature Importance (XGBoost)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIGURES_PATH / \"xgb_feature_importance.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Feature Importance (XGBoost):\")\n",
    "    print(importance_df.to_string(index=False))\n",
    "\n",
    "# Conclus√£o\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INSIGHTS E RECOMENDA√á√ïES\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "‚úÖ An√°lise conclu√≠da com sucesso!\n",
    "\n",
    "üìå PRINCIPAIS INSIGHTS:\n",
    "‚Ä¢ As vari√°veis relacionadas a transa√ß√µes (Total_Trans_Ct, Total_Trans_Amt) \n",
    "  t√™m alto poder preditivo para churn\n",
    "‚Ä¢ Mudan√ßas no comportamento transacional (Q4 vs Q1) s√£o indicadores importantes\n",
    "‚Ä¢ Vari√°veis derivadas (LTV_Proxy, Trans_Per_Month) agregam valor ao modelo\n",
    "\n",
    "üí° RECOMENDA√á√ïES PARA RETEN√á√ÉO:\n",
    "1. Monitorar clientes com queda abrupta no n√∫mero de transa√ß√µes\n",
    "2. Criar campanhas direcionadas para clientes com baixa atividade transacional\n",
    "3. Priorizar clientes com alto LTV_Proxy em programas de fidelidade\n",
    "4. Investigar mudan√ßas sazonais no comportamento (Q4 vs Q1)\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
